# train_model.py
import yfinance as yf
import numpy as np
import pandas as pd
import pickle
import json
from datetime import datetime
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

# ==================== CONFIGURA√á√ïES ====================
SYMBOL = 'DIS'
START_DATE = '2018-01-01'
END_DATE = '2024-07-20'
WINDOW_SIZE = 60
EPOCHS = 100
BATCH_SIZE = 32

# ==================== COLETA DE DADOS ====================
print("üìä Baixando dados da Disney...")
df = yf.download(SYMBOL, start=START_DATE, end=END_DATE, progress=False)
print(f"‚úÖ {len(df)} dias coletados")

# ==================== PREPARA√á√ÉO DOS DADOS ====================
print("\nüîß Preparando dados...")

# Usar apenas Close
close_prices = df['Close'].values.reshape(-1, 1)

# Normalizar
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(close_prices)

# Criar sequ√™ncias
X, y = [], []
for i in range(WINDOW_SIZE, len(scaled_data)):
    X.append(scaled_data[i-WINDOW_SIZE:i, 0])
    y.append(scaled_data[i, 0])

X = np.array(X)
y = np.array(y)
X = X.reshape(X.shape[0], X.shape[1], 1)

# Dividir dados: 70% treino, 15% valida√ß√£o, 15% teste
n_train = int(len(X) * 0.7)
n_val = int(len(X) * 0.85)

X_train, y_train = X[:n_train], y[:n_train]
X_val, y_val = X[n_train:n_val], y[n_train:n_val]
X_test, y_test = X[n_val:], y[n_val:]

datas = df.index[WINDOW_SIZE:]
datas_train = datas[:n_train]
datas_val = datas[n_train:n_val]
datas_test = datas[n_val:]

print(f"Treino: {len(X_train)} | Valida√ß√£o: {len(X_val)} | Teste: {len(X_test)}")

# ==================== CRIAR MODELO ====================
print("\nü§ñ Criando modelo LSTM...")

model = Sequential([
    LSTM(50, return_sequences=True, input_shape=(WINDOW_SIZE, 1)),
    Dropout(0.2),
    LSTM(50, return_sequences=False),
    Dropout(0.2),
    Dense(25),
    Dense(1)
])

model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])
print(f"Par√¢metros: {model.count_params()}")

# ==================== TREINAR MODELO ====================
print("\nüöÄ Treinando modelo...")

callbacks = [
    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001, verbose=1)
]

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=callbacks,
    verbose=1
)

# ==================== AVALIAR MODELO ====================
print("\nüìä Avaliando modelo...")

y_pred = model.predict(X_test, verbose=0)

# Inverter normaliza√ß√£o
y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))
y_pred_inv = scaler.inverse_transform(y_pred)

# Calcular m√©tricas
mae = mean_absolute_error(y_test_inv, y_pred_inv)
mse = mean_squared_error(y_test_inv, y_pred_inv)
rmse = np.sqrt(mse)
mape = np.mean(np.abs((y_test_inv - y_pred_inv) / y_test_inv)) * 100

direction_true = np.diff(y_test_inv.flatten()) > 0
direction_pred = np.diff(y_pred_inv.flatten()) > 0
direction_accuracy = np.mean(direction_true == direction_pred) * 100

metrics = {
    'mae': float(mae),
    'rmse': float(rmse),
    'mape': float(mape),
    'direction_accuracy': float(direction_accuracy)
}

print(f"\nMAE: ${mae:.2f}")
print(f"RMSE: ${rmse:.2f}")
print(f"MAPE: {mape:.2f}%")
print(f"Acur√°cia Dire√ß√£o: {direction_accuracy:.1f}%")

# ==================== VISUALIZA√á√ïES ====================
print("\nüìà Criando visualiza√ß√µes...")

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. Previs√£o vs Real
ax = axes[0, 0]
ax.plot(datas_test, y_test_inv, label='Real', alpha=0.7)
ax.plot(datas_test, y_pred_inv, label='Previs√£o', alpha=0.7)
ax.set_title('Previs√£o vs Valores Reais')
ax.set_xlabel('Data')
ax.set_ylabel('Pre√ßo ($)')
ax.legend()
ax.grid(True, alpha=0.3)

# 2. Hist√≥rico de Loss
ax = axes[0, 1]
ax.plot(history.history['loss'], label='Treino')
ax.plot(history.history['val_loss'], label='Valida√ß√£o')
ax.set_title('Hist√≥rico de Perda')
ax.set_xlabel('√âpoca')
ax.set_ylabel('Loss')
ax.legend()
ax.grid(True, alpha=0.3)

# 3. Scatter Plot
ax = axes[1, 0]
ax.scatter(y_test_inv, y_pred_inv, alpha=0.5)
ax.plot([y_test_inv.min(), y_test_inv.max()], 
        [y_test_inv.min(), y_test_inv.max()], 
        'r--', lw=2)
ax.set_title('Correla√ß√£o: Real vs Previs√£o')
ax.set_xlabel('Valor Real ($)')
ax.set_ylabel('Valor Previsto ($)')
ax.grid(True, alpha=0.3)

# 4. Distribui√ß√£o do Erro
ax = axes[1, 1]
erros = y_test_inv.flatten() - y_pred_inv.flatten()
ax.hist(erros, bins=30, edgecolor='black', alpha=0.7)
ax.axvline(x=0, color='r', linestyle='--')
ax.set_title('Distribui√ß√£o dos Erros')
ax.set_xlabel('Erro ($)')
ax.set_ylabel('Frequ√™ncia')
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('models/resultados_modelo.png', dpi=100)
print("‚úÖ Gr√°fico salvo: models/resultados_modelo.png")

# ==================== SALVAR MODELO E DADOS ====================
print("\nüíæ Salvando modelo e dados...")

import os
os.makedirs('models', exist_ok=True)
os.makedirs('data', exist_ok=True)

# Salvar modelo
model.save('models/modelo_disney_lstm.h5')

# Salvar m√©tricas
with open('models/metricas.json', 'w') as f:
    json.dump(metrics, f, indent=4)

# Salvar hist√≥rico
pd.DataFrame(history.history).to_csv('models/historico_treino.csv', index=False)

# Salvar configura√ß√£o
config = {
    'symbol': SYMBOL,
    'start_date': START_DATE,
    'end_date': END_DATE,
    'window_size': WINDOW_SIZE,
    'features': ['close'],
    'n_features': 1,
    'total_params': int(model.count_params()),
    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
    'epochs_trained': len(history.history['loss']),
    'best_val_loss': float(min(history.history['val_loss']))
}

with open('models/config.json', 'w') as f:
    json.dump(config, f, indent=4)

# Salvar dados preparados para API
dados_lstm = {
    'simples': {
        'X_train': X_train,
        'y_train': y_train,
        'X_val': X_val,
        'y_val': y_val,
        'X_test': X_test,
        'y_test': y_test,
        'datas_train': datas_train,
        'datas_val': datas_val,
        'datas_test': datas_test,
        'scaler': scaler
    }
}

with open('data/dados_lstm.pkl', 'wb') as f:
    pickle.dump(dados_lstm, f)

# Salvar dados processados
df_processed = df[['Open', 'High', 'Low', 'Close', 'Volume']].copy()
df_processed.reset_index(inplace=True)
df_processed.columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']
df_processed.to_csv('data/dados_processados.csv', index=False)

print("\n‚úÖ TREINAMENTO CONCLU√çDO!")
print(f"üìÅ Arquivos salvos:")
print(f"   ‚Ä¢ models/modelo_disney_lstm.h5")
print(f"   ‚Ä¢ models/metricas.json")
print(f"   ‚Ä¢ models/config.json")
print(f"   ‚Ä¢ models/historico_treino.csv")
print(f"   ‚Ä¢ models/resultados_modelo.png")
print(f"   ‚Ä¢ data/dados_lstm.pkl")
print(f"   ‚Ä¢ data/dados_processados.csv")
print(f"\nüéØ Scaler: {scaler.n_features_in_} feature(s)")
print(f"üéØ Modelo: input_shape = ({WINDOW_SIZE}, 1)")